{"metadata":{"availableInstances":[{"_defaultOrder":0,"_isFastLaunch":true,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":4,"name":"ml.t3.medium","vcpuNum":2},{"_defaultOrder":1,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.t3.large","vcpuNum":2},{"_defaultOrder":2,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.t3.xlarge","vcpuNum":4},{"_defaultOrder":3,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.t3.2xlarge","vcpuNum":8},{"_defaultOrder":4,"_isFastLaunch":true,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.m5.large","vcpuNum":2},{"_defaultOrder":5,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.m5.xlarge","vcpuNum":4},{"_defaultOrder":6,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.m5.2xlarge","vcpuNum":8},{"_defaultOrder":7,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.m5.4xlarge","vcpuNum":16},{"_defaultOrder":8,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.m5.8xlarge","vcpuNum":32},{"_defaultOrder":9,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.m5.12xlarge","vcpuNum":48},{"_defaultOrder":10,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.m5.16xlarge","vcpuNum":64},{"_defaultOrder":11,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.m5.24xlarge","vcpuNum":96},{"_defaultOrder":12,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.m5d.large","vcpuNum":2},{"_defaultOrder":13,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.m5d.xlarge","vcpuNum":4},{"_defaultOrder":14,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.m5d.2xlarge","vcpuNum":8},{"_defaultOrder":15,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.m5d.4xlarge","vcpuNum":16},{"_defaultOrder":16,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.m5d.8xlarge","vcpuNum":32},{"_defaultOrder":17,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.m5d.12xlarge","vcpuNum":48},{"_defaultOrder":18,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.m5d.16xlarge","vcpuNum":64},{"_defaultOrder":19,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.m5d.24xlarge","vcpuNum":96},{"_defaultOrder":20,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":true,"memoryGiB":0,"name":"ml.geospatial.interactive","supportedImageNames":["sagemaker-geospatial-v1-0"],"vcpuNum":0},{"_defaultOrder":21,"_isFastLaunch":true,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":4,"name":"ml.c5.large","vcpuNum":2},{"_defaultOrder":22,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.c5.xlarge","vcpuNum":4},{"_defaultOrder":23,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.c5.2xlarge","vcpuNum":8},{"_defaultOrder":24,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.c5.4xlarge","vcpuNum":16},{"_defaultOrder":25,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":72,"name":"ml.c5.9xlarge","vcpuNum":36},{"_defaultOrder":26,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":96,"name":"ml.c5.12xlarge","vcpuNum":48},{"_defaultOrder":27,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":144,"name":"ml.c5.18xlarge","vcpuNum":72},{"_defaultOrder":28,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.c5.24xlarge","vcpuNum":96},{"_defaultOrder":29,"_isFastLaunch":true,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.g4dn.xlarge","vcpuNum":4},{"_defaultOrder":30,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.g4dn.2xlarge","vcpuNum":8},{"_defaultOrder":31,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.g4dn.4xlarge","vcpuNum":16},{"_defaultOrder":32,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.g4dn.8xlarge","vcpuNum":32},{"_defaultOrder":33,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.g4dn.12xlarge","vcpuNum":48},{"_defaultOrder":34,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.g4dn.16xlarge","vcpuNum":64},{"_defaultOrder":35,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":61,"name":"ml.p3.2xlarge","vcpuNum":8},{"_defaultOrder":36,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":244,"name":"ml.p3.8xlarge","vcpuNum":32},{"_defaultOrder":37,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":488,"name":"ml.p3.16xlarge","vcpuNum":64},{"_defaultOrder":38,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.p3dn.24xlarge","vcpuNum":96},{"_defaultOrder":39,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.r5.large","vcpuNum":2},{"_defaultOrder":40,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.r5.xlarge","vcpuNum":4},{"_defaultOrder":41,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.r5.2xlarge","vcpuNum":8},{"_defaultOrder":42,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.r5.4xlarge","vcpuNum":16},{"_defaultOrder":43,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.r5.8xlarge","vcpuNum":32},{"_defaultOrder":44,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.r5.12xlarge","vcpuNum":48},{"_defaultOrder":45,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":512,"name":"ml.r5.16xlarge","vcpuNum":64},{"_defaultOrder":46,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.r5.24xlarge","vcpuNum":96},{"_defaultOrder":47,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.g5.xlarge","vcpuNum":4},{"_defaultOrder":48,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.g5.2xlarge","vcpuNum":8},{"_defaultOrder":49,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.g5.4xlarge","vcpuNum":16},{"_defaultOrder":50,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.g5.8xlarge","vcpuNum":32},{"_defaultOrder":51,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.g5.16xlarge","vcpuNum":64},{"_defaultOrder":52,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.g5.12xlarge","vcpuNum":48},{"_defaultOrder":53,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.g5.24xlarge","vcpuNum":96},{"_defaultOrder":54,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.g5.48xlarge","vcpuNum":192},{"_defaultOrder":55,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":1152,"name":"ml.p4d.24xlarge","vcpuNum":96},{"_defaultOrder":56,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":1152,"name":"ml.p4de.24xlarge","vcpuNum":96}],"instance_type":"ml.m5.2xlarge","kernelspec":{"display_name":"Python 3 (Data Science)","language":"python","name":"python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# | NLP | PPO | DialogSum | Less-Toxic Summarize |\n\n## NLP (Natural Language Processing) with PEFT (Parameter Efficient Fine-Tuning) and LoRA (Low-Rank Adaptation) for Less-Toxic Summarization\n\n# <b>1 <span style='color:#78D118'>|</span> Introduction</b>\n\nThis project explores the capabilities of Large Language Models (LLMs), particularly emphasizing the utilization of Parameter Efficient Fine-Tuning (PEFT) to create dialogue summaries with reduced toxicity. We achieve this by employing the FLAN-T5 model alongside Meta AI's hate speech reward model.\n\nOur primary objective is to improve the quality of dialogue summaries while minimizing toxicity. To attain this, we apply Proximal Policy Optimization (PPO) for fine-tuning, aiming to mitigate the model's toxic output. Furthermore, we will showcase the advantages of Parameter Efficient Fine-Tuning (PEFT), illustrating that its benefits surpass any potential minor performance trade-offs.\n\n - NOTE: This is an example and we not using the entirety of the data used.\n \n## Objectives :\n - Train LLM to make less toxic dialogue summarization.\n \n \n ## The DialogSum Dataset:\nThe [DialogSum Dataset](https://huggingface.co/datasets/knkarthick/dialogsum) DialogSum is a large-scale dialogue summarization dataset, consisting of 13,460 (Plus 100 holdout data for topic generation) dialogues with corresponding manually labeled summaries and topics.\n\n## Project Workflow:\n\n- **Setup**: Import necessary libraries and define project parameters.\n- **Dataset Exploration**: Discovering DialogSum Dataset.\n- **Test Model Zero Shot Inferencing**: Initially, test the FLAN-T5 model for zero-shot inferencing on dialogue summarization tasks to establish a baseline performance.\n- **Dataset Preprocess Dialog and Summary**: Preprocess the dialog and its corresponding summary from the dataset to prepare for the train.\n-  **Perform Parameter Efficient Fine-Tuning (PEFT)**: Implement Parameter Efficient Fine-Tuning (PEFT), a more efficient fine-tuning approach that can significantly reduce training time while maintaining performance.\n-  **Evaluation**:\n    - Perform human evaluation to gauge the model's output in terms of readability and coherence. This can involve annotators ranking generated summaries for quality.\n    - Utilize ROUGE metrics to assess the quality of the generated summaries. ROUGE measures the overlap between generated summaries and human-written references.\n\n# <b>2<span style='color:#78D118'>|</span> Setup</b>\n## <b>2.1 <span style='color:#78D118'>|</span> Imports</b>","metadata":{}},{"cell_type":"code","source":"%pip install --upgrade pip\n%pip install --disable-pip-version-check \\\n    torch==1.13.1 \\\n    torchdata==0.5.1 --quiet\n\n%pip install \\\n    transformers==4.27.2 \\\n    datasets==2.11.0 \\\n    evaluate==0.4.0 \\\n    rouge_score==0.1.2 \\\n    peft==0.3.0 --quiet\n\n# Installing the Reinforcement Learning library directly from github.\n%pip install git+https://github.com/lvwerra/trl.git@25fa1bd    ","metadata":{"tags":[],"jupyter":{"source_hidden":true,"outputs_hidden":true},"_kg_hide-input":true,"_kg_hide-output":true,"collapsed":true},"execution_count":2,"outputs":[{"name":"stdout","output_type":"stream","text":"Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (23.2.1)\n\n\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n\n\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n\n\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n\npytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\n\npytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\n\nspyder 4.0.1 requires pyqt5<5.13; python_version >= \"3\", which is not installed.\n\nspyder 4.0.1 requires pyqtwebengine<5.13; python_version >= \"3\", which is not installed.\n\nnotebook 6.5.5 requires pyzmq<25,>=17, but you have pyzmq 25.1.1 which is incompatible.\n\npathos 0.3.1 requires dill>=0.3.7, but you have dill 0.3.6 which is incompatible.\n\npathos 0.3.1 requires multiprocess>=0.70.15, but you have multiprocess 0.70.14 which is incompatible.\n\nsparkmagic 0.20.4 requires nest-asyncio==1.5.5, but you have nest-asyncio 1.5.7 which is incompatible.\n\nspyder 4.0.1 requires jedi==0.14.1, but you have jedi 0.19.0 which is incompatible.\u001b[0m\u001b[31m\n\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n\nCollecting git+https://github.com/lvwerra/trl.git@25fa1bd\n\n  Cloning https://github.com/lvwerra/trl.git (to revision 25fa1bd) to /tmp/pip-req-build-8qt9icdc\n\n  Running command git clone --filter=blob:none --quiet https://github.com/lvwerra/trl.git /tmp/pip-req-build-8qt9icdc\n\n\u001b[33m  WARNING: Did not find branch or tag '25fa1bd', assuming revision or ref.\u001b[0m\u001b[33m\n\n\u001b[0m  Running command git checkout -q 25fa1bd\n\n  Resolved https://github.com/lvwerra/trl.git to commit 25fa1bd\n\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\n\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from trl==0.4.2.dev0) (1.13.1)\n\nRequirement already satisfied: transformers>=4.18.0 in /opt/conda/lib/python3.7/site-packages (from trl==0.4.2.dev0) (4.27.2)\n\nRequirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.7/site-packages (from trl==0.4.2.dev0) (1.21.6)\n\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.7/site-packages (from trl==0.4.2.dev0) (0.20.3)\n\nRequirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (from trl==0.4.2.dev0) (2.11.0)\n\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4.0->trl==0.4.2.dev0) (4.7.1)\n\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch>=1.4.0->trl==0.4.2.dev0) (11.7.99)\n\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.7/site-packages (from torch>=1.4.0->trl==0.4.2.dev0) (8.5.0.96)\n\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.7/site-packages (from torch>=1.4.0->trl==0.4.2.dev0) (11.10.3.66)\n\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch>=1.4.0->trl==0.4.2.dev0) (11.7.99)\n\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->trl==0.4.2.dev0) (65.5.1)\n\nRequirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->trl==0.4.2.dev0) (0.41.1)\n\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (3.0.12)\n\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (0.16.4)\n\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (23.2)\n\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (6.0.1)\n\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (2023.8.8)\n\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (2.31.0)\n\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (0.13.3)\n\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (4.66.1)\n\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (6.7.0)\n\nRequirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from accelerate->trl==0.4.2.dev0) (5.6.7)\n\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets->trl==0.4.2.dev0) (12.0.1)\n\nRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from datasets->trl==0.4.2.dev0) (0.3.6)\n\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets->trl==0.4.2.dev0) (1.3.5)\n\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets->trl==0.4.2.dev0) (3.4.1)\n\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets->trl==0.4.2.dev0) (0.70.14)\n\nRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.7/site-packages (from datasets->trl==0.4.2.dev0) (2023.1.0)\n\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets->trl==0.4.2.dev0) (3.8.5)\n\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets->trl==0.4.2.dev0) (0.18.0)\n\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (23.1.0)\n\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (2.0.4)\n\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (6.0.4)\n\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (4.0.3)\n\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (1.9.2)\n\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (1.3.3)\n\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (1.3.1)\n\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (0.13.0)\n\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (2.8)\n\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (2.0.4)\n\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (2023.7.22)\n\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers>=4.18.0->trl==0.4.2.dev0) (2.2.0)\n\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets->trl==0.4.2.dev0) (2.8.2)\n\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets->trl==0.4.2.dev0) (2019.3)\n\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets->trl==0.4.2.dev0) (1.14.0)\n\nBuilding wheels for collected packages: trl\n\n  Building wheel for trl (setup.py) ... \u001b[?25ldone\n\n\u001b[?25h  Created wheel for trl: filename=trl-0.4.2.dev0-py3-none-any.whl size=67532 sha256=7d37de49afb570ef259c128f4fa607aaa5d9bdf18c3c0bcd6005c876b43257ce\n\n  Stored in directory: /tmp/pip-ephem-wheel-cache-mskj42xa/wheels/41/26/75/08a45cee1a1bba06c4f340451483cdfe150f4c8dad3876fb2e\n\nSuccessfully built trl\n\n\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n\n\u001b[0mInstalling collected packages: trl\n\nSuccessfully installed trl-0.4.2.dev0\n\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"}]},{"cell_type":"code","source":"from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM, GenerationConfig\nfrom datasets import load_dataset\nfrom peft import PeftModel, PeftConfig, LoraConfig, TaskType\n\n# trl: Transformer Reinforcement Learning library\nfrom trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead\nfrom trl import create_reference_model\nfrom trl.core import LengthSampler\n\nimport torch\nimport evaluate\n\nimport numpy as np\nimport pandas as pd\n\n# tqdm library makes the loops show a smart progress meter.\nfrom tqdm import tqdm\ntqdm.pandas()","metadata":{"tags":[],"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"model_name=\"google/flan-t5-base\"\nhuggingface_dataset_name = \"knkarthick/dialogsum\"\n\ndataset_original = load_dataset(huggingface_dataset_name)","metadata":{"tags":[],"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"247eb4fa45294e4c90527b907688c94d","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/4.65k [00:00<?, ?B/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"Downloading and preparing dataset csv/knkarthick--dialogsum to /root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-3005b557c2c04c1d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"815a8df5c07f45679f2383fdae890a6d","version_major":2,"version_minor":0},"text/plain":["Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"00b2df3fc7574c6cab4b04111c1d5a12","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/11.3M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"faa94c6bd72e4f78a64565efe8203742","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"de78bff2f22840c79eb40e82fd14c937","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/442k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6bdd81d2e89b48a39876d2a0e7a73105","version_major":2,"version_minor":0},"text/plain":["Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Generating test split: 0 examples [00:00, ? examples/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Generating validation split: 0 examples [00:00, ? examples/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-3005b557c2c04c1d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"557ca088db984b248efb7734081f4951","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'dialogue', 'summary', 'topic'],\n","        num_rows: 12460\n","    })\n","    test: Dataset({\n","        features: ['id', 'dialogue', 'summary', 'topic'],\n","        num_rows: 1500\n","    })\n","    validation: Dataset({\n","        features: ['id', 'dialogue', 'summary', 'topic'],\n","        num_rows: 500\n","    })\n","})"]},"metadata":{}}]},{"cell_type":"markdown","source":"## <b>2.2 <span style='color:#78D118'>|</span> Methods</b>","metadata":{}},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"\\ntrainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"","metadata":{"tags":[],"_kg_hide-input":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# <b>3<span style='color:#78D118'>|</span> Tokenize the Data</b>\n","metadata":{}},{"cell_type":"markdown","source":"The next step involves dataset preprocessing. We'll select a subset of the data, filter dialogues to a specific length to ensure readability while maintaining meaningful content, and then integrate each dialogue with an instruction before tokenizing the prompts. The resulting token IDs will be stored in the `input_ids` field, while the decoded prompts will be saved in the `query` field.\n\nTo streamline this process, it's advisable to create a function called `build_dataset`. This function can be defined as follows:","metadata":{}},{"cell_type":"code","source":"def build_dataset(model_name,\n                  dataset_name,\n                  input_min_text_length, \n                  input_max_text_length):\n\n    \"\"\"\n    Preprocess the dataset and split it into train and test parts.\n\n    Parameters:\n    - model_name (str): Tokenizer model name.\n    - dataset_name (str): Name of the dataset to load.\n    - input_min_text_length (int): Minimum length of the dialogues.\n    - input_max_text_length (int): Maximum length of the dialogues.\n        \n    Returns:\n    - dataset_splits (datasets.dataset_dict.DatasetDict): Preprocessed dataset containing train and test parts.\n    \"\"\"\n    \n    # load dataset (only \"train\" part will be enough for this lab).\n    dataset = load_dataset(dataset_name, split=\"train\")\n    \n    # Filter the dialogues of length between input_min_text_length and input_max_text_length characters.\n    dataset = dataset.filter(lambda x: len(x[\"dialogue\"]) > input_min_text_length and len(x[\"dialogue\"]) <= input_max_text_length, batched=False)\n\n    # Prepare tokenizer. Setting device_map=\"auto\" allows to switch between GPU and CPU automatically.\n    tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")\n    \n    def tokenize(sample):\n        \n        # Wrap each dialogue with the instruction.\n        prompt = f\"\"\"\nSummarize the following conversation.\n\n{sample[\"dialogue\"]}\n\nSummary:\n\"\"\"\n        sample[\"input_ids\"] = tokenizer.encode(prompt)\n        \n        # This must be called \"query\", which is a requirement of our PPO library.\n        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n        return sample\n\n    # Tokenize each dialogue.\n    dataset = dataset.map(tokenize, batched=False)\n    dataset.set_format(type=\"torch\")\n    \n    # Split the dataset into train and test parts.\n    dataset_splits = dataset.train_test_split(test_size=0.2, shuffle=False, seed=42)\n\n    return dataset_splits\n\ndataset = build_dataset(model_name=model_name,\n                        dataset_name=huggingface_dataset_name,\n                        input_min_text_length=200, \n                        input_max_text_length=1000)\n\nprint(dataset)","metadata":{"tags":[],"_kg_hide-input":true,"_kg_hide-output":true},"execution_count":5,"outputs":[{"name":"stderr","output_type":"stream","text":"Found cached dataset csv (/root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-3005b557c2c04c1d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/12460 [00:00<?, ? examples/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ac50648a934b4d9383719b79557054f6","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"459d8edd92884209b76c847cb3066792","version_major":2,"version_minor":0},"text/plain":["Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cd9bc7649fe54067b11292c0d06340ed","version_major":2,"version_minor":0},"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"af313c0024254ab088ffe573acc35dbd","version_major":2,"version_minor":0},"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/10022 [00:00<?, ? examples/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"DatasetDict({\n\n    train: Dataset({\n\n        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'query'],\n\n        num_rows: 8017\n\n    })\n\n    test: Dataset({\n\n        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'query'],\n\n        num_rows: 2005\n\n    })\n\n})\n"}]},{"cell_type":"markdown","source":"# <b>4 <span style='color:#78D118'>|</span>  FLAN-T5 Model Fine-Tuned with Summarization Instruction</b>\n\n## <b>4.1 <span style='color:#78D118'>|</span>  Enhancing FLAN-T5 Model Fine-Tuned with Summarization Adapter</b>\n\nWe are enhancing the original FLAN-T5 model by adding a summarization adapter. This adapter is designed to improve the model's performance in summarization tasks.\n\nWe begin by configuring the adapter using the following parameters:\n- `r`: Rank, which is set to 32.\n- `lora_alpha`: LORA alpha value, set to 32.\n- `target_modules`: We specify the target modules as [\"q\", \"v\"].\n- `lora_dropout`: Dropout rate for LORA, set to 0.05.\n- `bias`: We use \"none\" as the bias configuration.\n- `task_type`: The task type is set to SEQ_2_SEQ_LM, which is suitable for FLAN-T5.\n\nNext, we load the pre-trained FLAN-T5 model and create an instance of the AutoModelForSeq2SeqLM with the specified model name and data type (torch_dtype).\n\nWe also create a PeftModel by incorporating the previously loaded model. \nAdditionally, we provide the LORA configuration, torch data type, device mapping, and specify that the model is trainable.","metadata":{}},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=32, # Rank\n    lora_alpha=32,\n    target_modules=[\"q\", \"v\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name, \n                                              torch_dtype=torch.bfloat16)\n\npeft_model = PeftModel.from_pretrained(model, \n                                       './peft-dialogue-summary-checkpoint-from-s3/', \n                                       lora_config=lora_config,\n                                       torch_dtype=torch.bfloat16, \n                                       device_map=\"auto\",                                       \n                                       is_trainable=True)\n\nprint(f'PEFT model parameters to be updated:\\n{print_number_of_trainable_model_parameters(peft_model)}\\n')\n","metadata":{"tags":[],"_kg_hide-input":true,"_kg_hide-output":false},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"18d29f264e5b42268e96bd339bd8674c","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b24ae99e7f71477fbb68524afcc1037c","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"967ae86897ad4bd7b77d89f85c1086ca","version_major":2,"version_minor":0},"text/plain":["Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"PEFT model parameters to be updated:\n\n\n\ntrainable model parameters: 3538944\n\nall model parameters: 251116800\n\npercentage of trainable model parameters: 1.41%\n\n\n"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## <b>4.2 <span style='color:#78D118'>|</span>  Enhancing LLM Summarization with Reinforcement Learning with POO</b>\n\nNow, we are in the process of preparing for fine-tuning the Language Model (LLM) using Reinforcement Learning (RL). Although a more detailed explanation of RL, our current focus is on setting up the Proximal Policy Optimization (PPO) model. \n\nThis PPO model will receive the instruction-fine-tuned PEFT model as input and will be utilized to optimize the RL policy in accordance with the reward model.","metadata":{}},{"cell_type":"code","source":"ppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(peft_model,                                                               \n                                                               torch_dtype=torch.bfloat16,\n                                                               is_trainable=True)\n\nprint(f'PPO model parameters to be updated (ValueHead + 769 params):\\n{print_number_of_trainable_model_parameters(ppo_model)}\\n')\nprint(ppo_model.v_head)","metadata":{"tags":[],"_kg_hide-input":true},"execution_count":10,"outputs":[{"name":"stdout","output_type":"stream","text":"PPO model parameters to be updated (ValueHead + 769 params):\n\n\n\ntrainable model parameters: 3539713\n\nall model parameters: 251117569\n\npercentage of trainable model parameters: 1.41%\n\n\n\nValueHead(\n\n  (dropout): Dropout(p=0.1, inplace=False)\n\n  (summary): Linear(in_features=768, out_features=1, bias=True)\n\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n\n)\n"}]},{"cell_type":"markdown","source":"During the Proximal Policy Optimization (PPO) process, only a subset of parameters will be updated, specifically those associated with the `ValueHead`. You can find more detailed information about this class of models in the [documentation](https://huggingface.co/docs/trl/main/en/models#trl.create_reference_model). The number of trainable parameters in the `ValueHead` can be computed as $(n+1) \\cdot m$, where $n$ represents the number of input units (in this case, $n=768$) and $m$ represents the number of output units (which is $m=1$ in this context). The additional $+1$ term in the equation accounts for the bias term.\n\nNow, let's create a frozen copy of the PPO model, which will serve as a reference model. This reference model will represent the Language Model (LLM) before detoxification. Importantly, none of the parameters of the reference model will be updated during PPO training. This is by design.","metadata":{}},{"cell_type":"code","source":"ref_model = create_reference_model(ppo_model)\n\nprint(f'Reference model parameters to be updated:\\n{print_number_of_trainable_model_parameters(ref_model)}\\n')","metadata":{"tags":[],"_kg_hide-input":true},"execution_count":11,"outputs":[{"name":"stdout","output_type":"stream","text":"Reference model parameters to be updated:\n\n\n\ntrainable model parameters: 0\n\nall model parameters: 251117569\n\npercentage of trainable model parameters: 0.00%\n\n\n"}]},{"cell_type":"markdown","source":"# <b>5<span style='color:#78D118'>|</span> Building a Reward Model for Reinforcement Learning</b>\n\n**Reinforcement Learning (RL)** stands as a pivotal branch of machine learning wherein agents make decisions within an environment to maximize their cumulative rewards. The behavior of these agents is governed by a decision-making **policy**, and the fundamental objective of RL is for the agent to acquire an optimal or near-optimal policy that maximizes the **reward function**.\n\nPreviously, the original policy was rooted in the instruct PEFT model – essentially, the Language Model (LLM) before undergoing detoxification. While one approach involved soliciting human labelers to provide feedback on the toxicity of the model's outputs, this process can become prohibitively costly when applied throughout the entire fine-tuning phase. A pragmatic solution to circumvent this expense is to implement a reward model that encourages the agent to produce detoxified dialogue summaries.\n\nA sensible approach here is to perform sentiment analysis on the model's outputs, classifying them into two categories: `nothate` and `hate`. Higher rewards are assigned when the likelihood of classifying an output as `nothate` is greater.\n\nIn this context, we will employ [Meta AI's RoBERTa-based hate speech model](https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target) as our reward model. This model generates **logits** and subsequently predicts probabilities for two classes: `nothate` and `hate`. Positive rewards are derived from the logits associated with the `nothate` class. The model will undergo further fine-tuning using Proximal Policy Optimization (PPO) with these reward values.\n\n## <b>5.1<span style='color:#78D118'>|</span> Load Meta AI's RoBERTa-based hate speech model</b>","metadata":{}},{"cell_type":"code","source":"toxicity_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\ntoxicity_tokenizer = AutoTokenizer.from_pretrained(toxicity_model_name, device_map=\"auto\")\ntoxicity_model = AutoModelForSequenceClassification.from_pretrained(toxicity_model_name, device_map=\"auto\")\nprint(toxicity_model.config.id2label)","metadata":{"tags":[],"_kg_hide-input":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"039a3158ffd64ab29bba27ff01cfd8ed","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"70af5114b17a46e3b919019d1b95bcd5","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ff435381497c4822812deb134e0a278b","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7f139c6b1f904c65a1ace2688b5fcaba","version_major":2,"version_minor":0},"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5dbc8903be264a12872aa8bfe7d8354c","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/816 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"47ac1cafe7734e078212f6af323159c0","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"},{"name":"stdout","output_type":"stream","text":"{0: 'nothate', 1: 'hate'}\n"}]},{"cell_type":"markdown","source":"Take some non-toxic text, tokenize it, and pass it to the model. Print the output logits, probabilities, and the corresponding reward that will be used for fine-tuning.","metadata":{"tags":[]}},{"cell_type":"code","source":"non_toxic_text = \"#Person 1# tells Tommy that he didn't like the movie.\"\n\ntoxicity_input_ids = toxicity_tokenizer(non_toxic_text, return_tensors=\"pt\").input_ids\n\nlogits = toxicity_model(input_ids=toxicity_input_ids).logits\nprint(f'logits [not hate, hate]: {logits.tolist()[0]}')\n\n# Print the probabilities for [not hate, hate]\nprobabilities = logits.softmax(dim=-1).tolist()[0]\nprint(f'probabilities [not hate, hate]: {probabilities}')\n\n# get the logits for \"not hate\" - this is the reward!\nnot_hate_index = 0\nnothate_reward = (logits[:, not_hate_index]).tolist()\nprint(f'reward (high): {nothate_reward}')","metadata":{"tags":[],"_kg_hide-input":true},"execution_count":13,"outputs":[{"name":"stdout","output_type":"stream","text":"logits [not hate, hate]: [3.114100694656372, -2.4896175861358643]\n\nprobabilities [not hate, hate]: [0.9963293671607971, 0.003670616541057825]\n\nreward (high): [3.114100694656372]\n"}]},{"cell_type":"markdown","source":"Let's show a toxic comment.  This will have a low reward because it is more toxic.","metadata":{}},{"cell_type":"code","source":"toxic_text = \"#Person 1# tells Tommy that the movie was terrible, dumb and stupid.\"\n\ntoxicity_input_ids = toxicity_tokenizer(toxic_text, return_tensors=\"pt\").input_ids\n\nlogits = toxicity_model(toxicity_input_ids).logits\nprint(f'logits [not hate, hate]: {logits.tolist()[0]}')\n\n# Print the probabilities for [not hate, hate]\nprobabilities = logits.softmax(dim=-1).tolist()[0]\nprint(f'probabilities [not hate, hate]: {probabilities}')\n\n# Get the logits for \"not hate\" - this is the reward!\nnothate_reward = (logits[:, not_hate_index]).tolist() \nprint(f'reward (low): {nothate_reward}')","metadata":{"tags":[],"_kg_hide-input":true},"execution_count":14,"outputs":[{"name":"stdout","output_type":"stream","text":"logits [not hate, hate]: [-0.6921188831329346, 0.3722729980945587]\n\nprobabilities [not hate, hate]: [0.25647106766700745, 0.7435289621353149]\n\nreward (low): [-0.6921188831329346]\n"}]},{"cell_type":"markdown","source":"## <b>5.2<span style='color:#78D118'>|</span> Setup Pipeline toxicity reward model</b>","metadata":{}},{"cell_type":"markdown","source":"Setup Hugging Face inference pipeline to simplify the code for the toxicity reward model:","metadata":{}},{"cell_type":"code","source":"device = 0 if torch.cuda.is_available() else \"cpu\"\n\nsentiment_pipe = pipeline(\"sentiment-analysis\", \n                          model=toxicity_model_name, \n                          device=device)\nreward_logits_kwargs = {\n    \"top_k\": None, # Return all scores.\n    \"function_to_apply\": \"none\", # Set to \"none\" to retrieve raw logits.\n    \"batch_size\": 16\n}\n\nreward_probabilities_kwargs = {\n    \"top_k\": None, # Return all scores.\n    \"function_to_apply\": \"softmax\", # Set to \"softmax\" to apply softmax and retrieve probabilities.\n    \"batch_size\": 16\n}\n\nprint(\"Reward model output:\")\nprint(\"For non-toxic text\")\nprint(sentiment_pipe(non_toxic_text, **reward_logits_kwargs))\nprint(sentiment_pipe(non_toxic_text, **reward_probabilities_kwargs))\nprint(\"For toxic text\")\nprint(sentiment_pipe(toxic_text, **reward_logits_kwargs))\nprint(sentiment_pipe(toxic_text, **reward_probabilities_kwargs))","metadata":{"tags":[],"_kg_hide-input":true},"execution_count":15,"outputs":[{"name":"stdout","output_type":"stream","text":"Reward model output:\n\nFor non-toxic text\n\n[{'label': 'nothate', 'score': 3.114100694656372}, {'label': 'hate', 'score': -2.4896175861358643}]\n\n[{'label': 'nothate', 'score': 0.9963293671607971}, {'label': 'hate', 'score': 0.003670616541057825}]\n\nFor toxic text\n\n[{'label': 'hate', 'score': 0.3722729980945587}, {'label': 'nothate', 'score': -0.6921188831329346}]\n\n[{'label': 'hate', 'score': 0.7435289621353149}, {'label': 'nothate', 'score': 0.25647106766700745}]\n"}]},{"cell_type":"markdown","source":"The outputs are the logits for both `nothate` (positive) and `hate` (negative) classes. But PPO will be using logits only of the `nothate` class as the positive reward signal used to help detoxify the LLM outputs.","metadata":{}},{"cell_type":"code","source":"print(sentiment_pipe(non_toxic_text, **reward_logits_kwargs))\nprint(sentiment_pipe(non_toxic_text, **reward_probabilities_kwargs))","metadata":{"tags":[],"_kg_hide-input":true},"execution_count":16,"outputs":[{"name":"stdout","output_type":"stream","text":"[{'label': 'nothate', 'score': 3.114100694656372}, {'label': 'hate', 'score': -2.4896175861358643}]\n\n[{'label': 'nothate', 'score': 0.9963293671607971}, {'label': 'hate', 'score': 0.003670616541057825}]\n"}]},{"cell_type":"code","source":"print(sentiment_pipe(toxic_text, **reward_logits_kwargs))\nprint(sentiment_pipe(toxic_text, **reward_probabilities_kwargs))","metadata":{"tags":[],"_kg_hide-input":true},"execution_count":17,"outputs":[{"name":"stdout","output_type":"stream","text":"[{'label': 'hate', 'score': 0.3722729980945587}, {'label': 'nothate', 'score': -0.6921188831329346}]\n\n[{'label': 'hate', 'score': 0.7435289621353149}, {'label': 'nothate', 'score': 0.25647106766700745}]\n"}]},{"cell_type":"markdown","source":"## <b>5.3<span style='color:#78D118'>|</span> Evaluate Toxicity</b>\n\nTo assess the model's performance both before and after the fine-tuning and detoxification processes, it is essential to establish the toxicity evaluation metric. The toxicity score is represented as a decimal value ranging from 0 to 1, where 1 signifies the highest degree of toxicity.","metadata":{"tags":[]}},{"cell_type":"code","source":"toxicity_evaluator = evaluate.load(\"toxicity\", \n                                    toxicity_model_name,\n                                    module_type=\"measurement\",\n                                    toxic_label=\"hate\")","metadata":{"tags":[],"_kg_hide-input":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7432dbf3daf64fddb1f56a08aba0d3fe","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/6.08k [00:00<?, ?B/s]"]},"metadata":{}}]},{"cell_type":"markdown","source":"Try to calculate toxicity for the same sentences as in section [2.2](#2.2). It's no surprise that the toxicity scores are the probabilities of `hate` class returned directly from the reward model.","metadata":{"tags":[]}},{"cell_type":"code","source":"toxicity_score = toxicity_evaluator.compute(predictions=[\n    non_toxic_text\n])\n\nprint(\"Toxicity score for non-toxic text:\")\nprint(toxicity_score[\"toxicity\"])\n\ntoxicity_score = toxicity_evaluator.compute(predictions=[\n    toxic_text\n])\n\nprint(\"\\nToxicity score for toxic text:\")\nprint(toxicity_score[\"toxicity\"])","metadata":{"tags":[],"_kg_hide-input":true},"execution_count":19,"outputs":[{"name":"stdout","output_type":"stream","text":"Toxicity score for non-toxic text:\n\n[0.003670616541057825]\n\n\n\nToxicity score for toxic text:\n\n[0.7435289621353149]\n"}]},{"cell_type":"markdown","source":"This evaluator can be effectively employed to calculate the toxicity levels of the dialogues. \n\nTo accomplish this, you will need to provide several essential components, including the test dataset (`dataset[\"test\"]`), the tokenizer used in the aforementioned section, the previously frozen PEFT model, and the toxicity evaluator itself. For a streamlined and organized approach, it is recommended to encapsulate these necessary procedures within a dedicated function named `evaluate_toxicity`.","metadata":{"tags":[]}},{"cell_type":"code","source":"def evaluate_toxicity(model, \n                      toxicity_evaluator, \n                      tokenizer, \n                      dataset, \n                      num_samples):\n    \n    \"\"\"\n    Preprocess the dataset and split it into train and test parts.\n\n    Parameters:\n    - model (trl model): Model to be evaluated.\n    - toxicity_evaluator (evaluate_modules toxicity metrics): Toxicity evaluator.\n    - tokenizer (transformers tokenizer): Tokenizer to be used.\n    - dataset (dataset): Input dataset for the evaluation.\n    - num_samples (int): Maximum number of samples for the evaluation.\n        \n    Returns:\n    tuple: A tuple containing two numpy.float64 values:\n    - mean (numpy.float64): Mean of the samples toxicity.\n    - std (numpy.float64): Standard deviation of the samples toxicity.\n    \"\"\"\n\n    max_new_tokens=100\n\n    toxicities = []\n    input_texts = []\n    for i, sample in tqdm(enumerate(dataset)):\n        input_text = sample[\"query\"]\n\n        if i > num_samples:\n            break\n            \n        input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True).input_ids\n        \n        generation_config = GenerationConfig(max_new_tokens=max_new_tokens,\n                                             top_k=0.0,\n                                             top_p=1.0,\n                                             do_sample=True)\n\n        response_token_ids = model.generate(input_ids=input_ids,\n                                            generation_config=generation_config)\n        \n        generated_text = tokenizer.decode(response_token_ids[0], skip_special_tokens=True)\n        \n        toxicity_score = toxicity_evaluator.compute(predictions=[(input_text + \" \" + generated_text)])\n\n        toxicities.extend(toxicity_score[\"toxicity\"])\n\n    # Compute mean & std using np.\n    mean = np.mean(toxicities)\n    std = np.std(toxicities)\n        \n    return mean, std","metadata":{"tags":[],"_kg_hide-input":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"And now perform the calculation of the model toxicity before fine-tuning/detoxification:","metadata":{"tags":[]}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")\n\nmean_before_detoxification, std_before_detoxification = evaluate_toxicity(model=ref_model, \n                                                                          toxicity_evaluator=toxicity_evaluator, \n                                                                          tokenizer=tokenizer, \n                                                                          dataset=dataset[\"test\"], \n                                                                          num_samples=10)\n\nprint(f'toxicity [mean, std] before detox: [{mean_before_detoxification}, {std_before_detoxification}]')","metadata":{"tags":[],"_kg_hide-input":true},"execution_count":21,"outputs":[{"name":"stderr","output_type":"stream","text":"11it [00:24,  2.25s/it]"},{"name":"stdout","output_type":"stream","text":"toxicity [mean, std] before detox: [0.03872112058293582, 0.03225256283112844]\n"},{"name":"stderr","output_type":"stream","text":"\n"}]},{"cell_type":"markdown","source":"## <b>6 <span style='color:#78D118'>|</span> Perform Fine-Tuning to Detoxify the Summaries</b>\n\nOptimize a RL policy against the reward model using Proximal Policy Optimization (PPO).","metadata":{}},{"cell_type":"markdown","source":"## <b>6.1 <span style='color:#78D118'>|</span> Initialize `PPOTrainer`</b>\n\nFor the `PPOTrainer` initialization, you will need a collator. Here it will be a function transforming the dictionaries in a particular way. You can define and test it:","metadata":{}},{"cell_type":"code","source":"def collator(data):\n    return dict((key, [d[key] for d in data]) for key in data[0])\n\ntest_data = [{\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\"}]\nprint(f'Collator input: {test_data}')\nprint(f'Collator output: {collator(test_data)}')","metadata":{"tags":[],"_kg_hide-input":true},"execution_count":22,"outputs":[{"name":"stdout","output_type":"stream","text":"Collator input: [{'key1': 'value1', 'key2': 'value2', 'key3': 'value3'}]\n\nCollator output: {'key1': ['value1'], 'key2': ['value2'], 'key3': ['value3']}\n"}]},{"cell_type":"markdown","source":"Configure the essential parameters. Load the `ppo_model` and the corresponding tokenizer. \n\nAdditionally, load a static version of the model, referred to as `ref_model`. \n\nThe purpose of having two models is twofold: the first model, `ppo_model`, undergoes optimization, while the second model, `ref_model`, functions as a reference point to compute the KL-divergence from the initial state. \n\nThis serves as an additional reward signal in the PPO training process, ensuring that the optimized model does not stray too far from the original Language Model (LLM).","metadata":{}},{"cell_type":"code","source":"learning_rate=1.41e-5\nmax_ppo_epochs=1\nmini_batch_size=4\nbatch_size=16\n\nconfig = PPOConfig(\n    model_name=model_name,    \n    learning_rate=learning_rate,\n    ppo_epochs=max_ppo_epochs,\n    mini_batch_size=mini_batch_size,\n    batch_size=batch_size\n)\n\nppo_trainer = PPOTrainer(config=config, \n                         model=ppo_model, \n                         ref_model=ref_model, \n                         tokenizer=tokenizer, \n                         dataset=dataset[\"train\"], \n                         data_collator=collator)","metadata":{"tags":[],"_kg_hide-input":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## <b>6.2 <span style='color:#78D118'>|</span> Fine-Tune the Model</b>","metadata":{}},{"cell_type":"markdown","source":"The fine-tuning loop comprises the following key steps:\n\n1. Retrieve query responses from the policy Language Model (PEFT model).\n2. Determine the sentiments associated with the queries and responses using the hate speech RoBERTa model.\n3. Optimize the policy using Proximal Policy Optimization (PPO) with the triplet of inputs, which includes the query, response, and the associated reward.\n\nYou can confirm that the operation is successfully running by monitoring the following metrics:\n\n- `objective/kl`: Minimization of the Kullback-Leibler (KL) divergence.\n- `ppo/returns/mean`: Maximization of the mean returns.\n- `ppo/policy/advantages_mean`: Maximization of the mean advantages.\n\nThese metrics serve as indicators of the training process's progress and the achievement of specific objectives within the fine-tuning loop.","metadata":{}},{"cell_type":"code","source":"output_min_length = 100\noutput_max_length = 400\noutput_length_sampler = LengthSampler(output_min_length, output_max_length)\n\ngeneration_kwargs = {\n    \"min_length\": 5,\n    \"top_k\": 0.0,\n    \"top_p\": 1.0,\n    \"do_sample\": True\n}\n\nreward_kwargs = {\n    \"top_k\": None, # Return all scores.\n    \"function_to_apply\": \"none\", # You want the raw logits without softmax.\n    \"batch_size\": 16\n}\n\nmax_ppo_steps = 10\n\nfor step, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n    # Break when you reach max_steps.\n    if step >= max_ppo_steps:\n        break   \n\n    prompt_tensors = batch[\"input_ids\"]\n\n    # Get response from FLAN-T5/PEFT LLM.\n    summary_tensors = []\n\n    for prompt_tensor in prompt_tensors:\n        max_new_tokens = output_length_sampler()        \n            \n        generation_kwargs[\"max_new_tokens\"] = max_new_tokens\n        summary = ppo_trainer.generate(prompt_tensor, **generation_kwargs)\n        \n        summary_tensors.append(summary.squeeze()[-max_new_tokens:])\n        \n    # This needs to be called \"response\".\n    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in summary_tensors]\n\n    # Compute reward outputs.\n    query_response_pairs = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]    \n    rewards = sentiment_pipe(query_response_pairs, **reward_kwargs)\n\n    # You use the `nothate` item because this is the score for the positive `nothate` class.\n    reward_tensors = [torch.tensor(reward[not_hate_index][\"score\"]) for reward in rewards]    \n\n    # Run PPO step.\n    stats = ppo_trainer.step(prompt_tensors, summary_tensors, reward_tensors)\n    ppo_trainer.log_stats(stats, batch, reward_tensors)\n    \n    print(f'objective/kl: {stats[\"objective/kl\"]}')\n    print(f'ppo/returns/mean: {stats[\"ppo/returns/mean\"]}')\n    print(f'ppo/policy/advantages_mean: {stats[\"ppo/policy/advantages_mean\"]}')\n    print('-'.join('' for x in range(100)))","metadata":{"tags":[],"_kg_hide-input":true},"execution_count":24,"outputs":[{"name":"stderr","output_type":"stream","text":"0it [00:00, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n\n1it [01:43, 103.30s/it]"},{"name":"stdout","output_type":"stream","text":"objective/kl: 29.314075469970703\n\nppo/returns/mean: -0.4844372570514679\n\nppo/policy/advantages_mean: -4.632137340365716e-09\n\n---------------------------------------------------------------------------------------------------\n"},{"name":"stderr","output_type":"stream","text":"2it [03:21, 100.19s/it]"},{"name":"stdout","output_type":"stream","text":"objective/kl: 35.85022735595703\n\nppo/returns/mean: -0.8316176533699036\n\nppo/policy/advantages_mean: -9.384150345681519e-09\n\n---------------------------------------------------------------------------------------------------\n"},{"name":"stderr","output_type":"stream","text":"3it [04:53, 96.61s/it] "},{"name":"stdout","output_type":"stream","text":"objective/kl: 31.081266403198242\n\nppo/returns/mean: -0.6446913480758667\n\nppo/policy/advantages_mean: -8.428234110624544e-09\n\n---------------------------------------------------------------------------------------------------\n"},{"name":"stderr","output_type":"stream","text":"4it [06:16, 91.12s/it]"},{"name":"stdout","output_type":"stream","text":"objective/kl: 22.59747886657715\n\nppo/returns/mean: -0.25419875979423523\n\nppo/policy/advantages_mean: 2.2152294221200464e-08\n\n---------------------------------------------------------------------------------------------------\n"},{"name":"stderr","output_type":"stream","text":"5it [07:47, 91.11s/it]"},{"name":"stdout","output_type":"stream","text":"objective/kl: 27.7932186126709\n\nppo/returns/mean: -0.32479071617126465\n\nppo/policy/advantages_mean: -2.324981540624549e-09\n\n---------------------------------------------------------------------------------------------------\n"},{"name":"stderr","output_type":"stream","text":"6it [09:34, 96.66s/it]"},{"name":"stdout","output_type":"stream","text":"objective/kl: 33.241607666015625\n\nppo/returns/mean: -0.6701866388320923\n\nppo/policy/advantages_mean: -9.555419566709134e-09\n\n---------------------------------------------------------------------------------------------------\n"},{"name":"stderr","output_type":"stream","text":"7it [11:06, 95.12s/it]"},{"name":"stdout","output_type":"stream","text":"objective/kl: 27.689035415649414\n\nppo/returns/mean: -0.46731656789779663\n\nppo/policy/advantages_mean: -6.645688443995823e-10\n\n---------------------------------------------------------------------------------------------------\n"},{"name":"stderr","output_type":"stream","text":"8it [12:33, 92.56s/it]"},{"name":"stdout","output_type":"stream","text":"objective/kl: 28.230976104736328\n\nppo/returns/mean: -0.4346347153186798\n\nppo/policy/advantages_mean: 6.4787522013887155e-09\n\n---------------------------------------------------------------------------------------------------\n"},{"name":"stderr","output_type":"stream","text":"9it [14:07, 92.81s/it]"},{"name":"stdout","output_type":"stream","text":"objective/kl: 26.905288696289062\n\nppo/returns/mean: -0.46542078256607056\n\nppo/policy/advantages_mean: -7.103521770801535e-09\n\n---------------------------------------------------------------------------------------------------\n"},{"name":"stderr","output_type":"stream","text":"10it [15:42, 94.29s/it]"},{"name":"stdout","output_type":"stream","text":"objective/kl: 27.23663902282715\n\nppo/returns/mean: -0.332530677318573\n\nppo/policy/advantages_mean: 1.497644319670144e-08\n\n---------------------------------------------------------------------------------------------------\n"},{"name":"stderr","output_type":"stream","text":"\n"}]},{"cell_type":"markdown","source":"## <b>6.3 <span style='color:#78D118'>|</span> Evaluate the Model Quantitatively</b>\n\n\nRetrieve the PPO/PEFT model from the saved disk checkpoint and employ the test dataset split to assess the toxicity score of the RL-fine-tuned model.","metadata":{}},{"cell_type":"code","source":"mean_after_detoxification, std_after_detoxification = evaluate_toxicity(model=ppo_model, \n                                                                        toxicity_evaluator=toxicity_evaluator, \n                                                                        tokenizer=tokenizer, \n                                                                        dataset=dataset[\"test\"], \n                                                                        num_samples=10)\nprint(f'toxicity [mean, std] after detox: [{mean_after_detoxification}, {std_after_detoxification}]')","metadata":{"tags":[],"_kg_hide-input":true},"execution_count":25,"outputs":[{"name":"stderr","output_type":"stream","text":"11it [00:21,  1.95s/it]"},{"name":"stdout","output_type":"stream","text":"toxicity [mean, std] after detox: [0.04065660611641678, 0.05703941539389816]\n"},{"name":"stderr","output_type":"stream","text":"\n"}]},{"cell_type":"markdown","source":"And compare the toxicity scores of the reference model (before detoxification) and fine-tuned model (after detoxification).","metadata":{"tags":[]}},{"cell_type":"code","source":"mean_improvement = (mean_before_detoxification - mean_after_detoxification) / mean_before_detoxification\nstd_improvement = (std_before_detoxification - std_after_detoxification) / std_before_detoxification\n\nprint(f'Percentage improvement of toxicity score after detoxification:')\nprint(f'mean: {mean_improvement*100:.2f}%')\nprint(f'std: {std_improvement*100:.2f}%')","metadata":{"tags":[],"_kg_hide-input":true},"execution_count":26,"outputs":[{"name":"stdout","output_type":"stream","text":"Percentage improvement of toxicity score after detoxification:\n\nmean: -5.00%\n\nstd: -76.85%\n"}]},{"cell_type":"markdown","source":"## <b>6.4 <span style='color:#78D118'>|</span> Evaluate the Model Qualitatively</b>\n\nExplore sample examples from the test dataset, allowing for a comparison between the initial `ref_model` and the fine-tuned/detoxified `ppo_model` using the toxicity evaluator.","metadata":{}},{"cell_type":"code","source":"batch_size = 20\ncompare_results = {}\n\ndf_batch = dataset[\"test\"][0:batch_size]\n\ncompare_results[\"query\"] = df_batch[\"query\"]\nprompt_tensors = df_batch[\"input_ids\"]\n\nsummary_tensors_ref = []\nsummary_tensors = []\n\n# Get response from ppo and base model.\nfor i in tqdm(range(batch_size)):\n    gen_len = output_length_sampler()\n    generation_kwargs[\"max_new_tokens\"] = gen_len\n    \n    summary = ref_model.generate(\n        input_ids=torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0).to(device), \n        **generation_kwargs\n    ).squeeze()[-gen_len:]\n    summary_tensors_ref.append(summary)\n\n    summary = ppo_model.generate(\n        input_ids=torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0).to(device), \n        **generation_kwargs\n    ).squeeze()[-gen_len:]\n    summary_tensors.append(summary)\n\n# Decode responses.\ncompare_results[\"response_before\"] = [tokenizer.decode(summary_tensors_ref[i]) for i in range(batch_size)]\ncompare_results[\"response_after\"] = [tokenizer.decode(summary_tensors[i]) for i in range(batch_size)]\n\n# Sentiment analysis of query/response pairs before/after.\ntexts_before = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_before\"])]\nrewards_before = sentiment_pipe(texts_before, **reward_kwargs)\ncompare_results[\"reward_before\"] = [reward[not_hate_index][\"score\"] for reward in rewards_before]\n\ntexts_after = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_after\"])]\nrewards_after = sentiment_pipe(texts_after, **reward_kwargs)\ncompare_results[\"reward_after\"] = [reward[not_hate_index][\"score\"] for reward in rewards_after]","metadata":{"tags":[],"_kg_hide-input":true},"execution_count":27,"outputs":[{"name":"stderr","output_type":"stream","text":"100%|██████████| 20/20 [01:24<00:00,  4.24s/it]\n"}]},{"cell_type":"markdown","source":"Store and review the results in a DataFrame","metadata":{"tags":[]}},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', 500)\ndf_compare_results = pd.DataFrame(compare_results)\ndf_compare_results[\"reward_diff\"] = df_compare_results['reward_after'] - df_compare_results['reward_before']\ndf_compare_results_sorted = df_compare_results.sort_values(by=['reward_diff'], ascending=False).reset_index(drop=True)\ndf_compare_results_sorted","metadata":{"tags":[],"_kg_hide-input":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>query</th>\n","      <th>response_before</th>\n","      <th>response_after</th>\n","      <th>reward_before</th>\n","      <th>reward_after</th>\n","      <th>reward_diff</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Summarize the following conversation. #Person1#: It smells like an ashtray in here! #Person2#: Hi honey! What's wrong? Why do you have that look on your face? #Person1#: What's wrong? I thought we agreed that you were gonna quit smoking. #Person2#: No! I said I was going to cut down which is very different. You can't just expect me to go cold turkey overnight! #Person1#: Look, there are other ways to quit. You can try the nicotine patch, or nicotine chewing gum. We spend a fortune on cigaret...</td>\n","      <td>&lt;pad&gt; Hopeless honey tells 6061# she has bad rape and #Person1# asks her to quit smoking because she doesn't have the willpower to do so. She said she'll keep going, but #Person1# tells her she will need a divorce.&lt;/s&gt;</td>\n","      <td>&lt;pad&gt; #Person1# thinks #Person2# smells like an ashtray because she doesn't know how to quit smoking and is too stressed to quit. #Person1# treats the situation embarrassingly.&lt;/s&gt;</td>\n","      <td>0.559593</td>\n","      <td>1.392192</td>\n","      <td>0.832600</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Summarize the following conversation. #Person1#: Judy, what is everybody talking about? #Person2#: Haven't you heard? Richard was fired by our manager. #Person1#: You're kidding. It can't be true. #Person2#: Believe it or not. Everybody is talking about it in the company. #Person1#: Really? I'm surprised. #Person2#: Me too. Summary: &lt;/s&gt;</td>\n","      <td>&lt;pad&gt; Judy's surprised by the news about Richard being fired by her manager. She tells Judy she's surprised.&lt;/s&gt;</td>\n","      <td>&lt;pad&gt; Judy wonders what people are talking about.&lt;/s&gt;</td>\n","      <td>1.384994</td>\n","      <td>1.858945</td>\n","      <td>0.473952</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Summarize the following conversation. #Person1#: Hello? #Person2#: Hello? #Person1#: Can I speak to Li Hong, please? #Person2#: Speaking. #Person1#: Hi, Li Hong. This is Alice. #Person2#: Hi, Alice. How are you? #Person1#: Not bad. Li Hong, I am sorry that I can't go to see Mrs. Brown with you tomorrow morning. My mother is ill. I must take care of her. #Person2#: I'm sorry to hear that. You'd better stay at home. After all, we can visit Mrs. Brown later #Person1#: OK. Bye - bye. #Person2#: ...</td>\n","      <td>&lt;pad&gt; Alice's mother's ill so she can't go to Mrs. Brown with Li Hong. Li Hong tells her to stay at home.&lt;/s&gt;</td>\n","      <td>&lt;pad&gt; Diane tells Li Hong that she can't go to see Mrs. Brown with Li Hong because she has a severe illness and is taking care of her mother.&lt;/s&gt;</td>\n","      <td>1.260455</td>\n","      <td>1.635229</td>\n","      <td>0.374774</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Summarize the following conversation. #Person1#: Could you help me, Sir? My flight got in 15 minutes ago. Everyone else has picked up the luggage but mine hasn't come through. #Person2#: I'm sorry, Madam, I'll go and find out if there is any more to come. Summary: &lt;/s&gt;</td>\n","      <td>&lt;pad&gt; #Person1# asks a man for help and the flight is 60 minutes delayed. They share the situation. #Person2# decides to go and see if there is any more to come.&lt;/s&gt;</td>\n","      <td>&lt;pad&gt; #Person1#'s flight got in 15 minutes ago, but while everyone else chipped in their luggage, #Person1#'s flight hasn't come through. #Person2#'ll ask her if there is anything else left.&lt;/s&gt;</td>\n","      <td>2.153149</td>\n","      <td>2.422932</td>\n","      <td>0.269783</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Summarize the following conversation. #Person1#: Amanda, how do you like this peaked cap? #Person2#: Didn't you say you want to buy a top hat? #Person1#: But I think this one fits me Well. Why don't you try on the sombrero in black? #Person2#: I don't like caps at all. Summary: &lt;/s&gt;</td>\n","      <td>&lt;pad&gt; Amanda loves a peaked cap. Then, Amanda asks #Person2# for a sombrero in black.&lt;/s&gt;</td>\n","      <td>&lt;pad&gt; Amanda showed #Person1# a peaked cap by #1 and she prefers a top hat.&lt;/s&gt;</td>\n","      <td>1.318374</td>\n","      <td>1.578068</td>\n","      <td>0.259693</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Summarize the following conversation. #Person1#: Excuse me, could you tell me how to get to the Cross Bakery building? #Person2#: The Cross Bakery building? Oh sure. You're actually walking in the opposite direction. #Person1#: Oh, you're kidding! I thought I was heading east. #Person2#: No, east is the other direction. To get to the Bakery, you need to turn around and go three blocks to Broadway. When you get to the intersection of Broadway and Elm, you hang a left. Go straight down that st...</td>\n","      <td>&lt;pad&gt; #Person1# asks #Person2# to show #Person1# the way to the Cross Bakery walking in the opposite direction. #Person2# tells #Person1# it's the opposite direction and shows #Person1# the way to the Bakery outside Bakery.&lt;/s&gt;</td>\n","      <td>&lt;pad&gt; #Person1# asks #Person2# how to get to the Cross Bakery building. To get to the Bakery, you have to walk three blocks to Broadway from Elm and turn right. To continue, you'd have to make a left when you hit Broadway.&lt;/s&gt;</td>\n","      <td>2.957855</td>\n","      <td>3.076255</td>\n","      <td>0.118400</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Summarize the following conversation. #Person1#: Where shall I register, please? #Person2#: Here. Do you have a registration card? #Person1#: Yes. Here you are. #Person2#: Please register your information here and pay for it. And I'll make a medical record for you. #Person1#: OK. How much do I need to pay for the registration? #Person2#: Please pay ten yuan for the registration. #Person1#: Here is my money. #Person2#: This is your registration card. Please don't lose it and bring it whenever...</td>\n","      <td>&lt;pad&gt; #Person1# asks for a registration. #Person2# gives #Person1# a registration code and in wong Yi from the pharmacy and tells #Person1# the way to the counseling room.&lt;/s&gt;</td>\n","      <td>&lt;pad&gt; #Person1# goes to the counseling room and is informed by #Person2# how to approach the consultation room.&lt;/s&gt;</td>\n","      <td>1.515073</td>\n","      <td>1.623435</td>\n","      <td>0.108362</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Summarize the following conversation. #Person1#: Hello. I want to reconfirm our flight to London. #Person2#: Yes, sir. Did you call the airline? #Person1#: Yes, I did. But I couldn't communicate with them in English. They speak only Spanish. So I need your help. #Person2#: Certainly, sir. What is the flight number and when are you leaving? #Person1#: We are taking IB 385 to London tomorrow at 1 p. m. #Person2#: Oh, I see, sir. We have the airline office inside the hotel. They have an English...</td>\n","      <td>&lt;pad&gt; #Person1# says #Person1# will take an IB 385 to London tomorrow, but can't call the airline in English. They awill ask 38 by calling 275 to confirm the flight.&lt;/s&gt;</td>\n","      <td>&lt;pad&gt; #Person1# calls a travel agency for a flight to London to make his request. #Person2# tells #Person1# #Person2# has informed the airline and they have an English-speaking staff.&lt;/s&gt;</td>\n","      <td>1.671294</td>\n","      <td>1.769185</td>\n","      <td>0.097891</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Summarize the following conversation. #Person1#: Oh, my God! What's this? #Person2#: What? #Person1#: Look! This window is open. #Person2#: Did you open it before we left? #Person1#: Are you kidding? It's winter. Why would I open it? #Person2#: I don't know. Wait. Is this yours? #Person1#: No! Oh, my God! Someone has broken into the house. #Person2#: It looks that way. That's probably why the door wasn't locked when we came in. #Person1#: I locked it when I left though. #Person2#: Yes, but t...</td>\n","      <td>&lt;pad&gt; Allen doubts Allen is old and he doesn't know what masked the burglar. They'll look upstairs to see if anyone had robbed the house. Allen confesses to #Person1# and tells #Person1# the TV's at the window and the food on the table. They do not think there will be someone yet.&lt;/s&gt;</td>\n","      <td>&lt;pad&gt; Allen thinks somebody has broken the house and doesn't want to let the robber in so he locked it before leaving. Allen thinks he forgot to lock the window the first time so he forgot to lock the door.&lt;/s&gt;</td>\n","      <td>2.079719</td>\n","      <td>2.171317</td>\n","      <td>0.091598</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Summarize the following conversation. #Person1#: I'm forming a music band. #Person2#: Do you already know how to play an instrument? #Person1#: Uh... Yeah! I'Ve told you a thousand times that I'm learning to play the drums. Now that I know how to play well, I would like to form a rock band. #Person2#: Aside from yourself, who are the other members of the band? #Person1#: We have a guy who plays guitar, and another who plays bass. Although we still haven't found anyone to be our singer. You t...</td>\n","      <td>&lt;pad&gt; #Person1# wants to form a rock band. #Person2# tells #Person1# the main bands are a guy and a singer. All the members of the band have musicians. They make auditions for the rock band and some pictures. #Person2# says #Person1# has some musical talent and invites #Person1#.&lt;/s&gt;</td>\n","      <td>&lt;pad&gt; #Person1# wants to form a music band. #Person1# wants to make a rock band but they don't have enough room. #Person2# wants to audition this weekend. Besides fraction, they have acoustics and are not a singer.&lt;/s&gt;</td>\n","      <td>2.842946</td>\n","      <td>2.930702</td>\n","      <td>0.087757</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Summarize the following conversation. #Person1#: I'd like to have this cashed, please. #Person2#: Please put you name and address here. May I see your passport? #Person1#: Yes. #Person2#: How would you like it? #Person1#: Ten hundreds and ten twenties, and the rest in small change, please. #Person2#: OK. Here you are. Summary: &lt;/s&gt;</td>\n","      <td>&lt;pad&gt; #Person1# wants to cheap car new. #Person2# needs to sell this unavailable car and #Person1# sends in the cash in large change for 10 hundreds and ten twenties.&lt;/s&gt;</td>\n","      <td>&lt;pad&gt; #Person1# wants to have this camshed and pay in small change. #Person2# has #Person1#'s name and address.&lt;/s&gt;</td>\n","      <td>1.692237</td>\n","      <td>1.738544</td>\n","      <td>0.046307</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Summarize the following conversation. #Person1#: What can I do for you, madam? #Person2#: I'd like to buy a toy car for my son. #Person1#: How about this one? #Person2#: It looks nice. How much is it? #Person1#: They're three hundred dollars. #Person2#: Oh, I'm afraid it's too expensive. Can you show me something cheaper? #Person1#: OK, This one is one hundred and twenty. It's the cheapest here. #Person2#: OK, I'll take it. Here's the money. #Person1#: Thank you very much. Summary: &lt;/s&gt;</td>\n","      <td>&lt;pad&gt; #Person1# wants to buy a toy car for #Person2#'s son, but #Person2#'s afraid the price is too expensive. #Person1# shows #Person2# the cheapest one.&lt;/s&gt;</td>\n","      <td>&lt;pad&gt; #Person1# offers #Person2# a toy car at a cheap price.&lt;/s&gt;</td>\n","      <td>1.333509</td>\n","      <td>1.363199</td>\n","      <td>0.029690</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>Summarize the following conversation. #Person1#: Today more and more families have personal computers. People have wider range of choice to communicate with the outside world. #Person2#: Right. With the establishment of Internet and a lot of web companies, people are getting more and more dependent on the web. #Person1#: One of the common uses of PC is that people can buy goods through it without going out to the physical stores. #Person2#: Can you tell me how it is done? #Person1#: If a cus...</td>\n","      <td>&lt;pad&gt; #Person1# tells #Person2# that many families have personal PCs. #Person2# teaches some practical ways for the customers to buy goods through the PC through the customers' invitation.&lt;/s&gt;</td>\n","      <td>&lt;pad&gt; #Person1# tells #Person2# how people can buy computers through the web, it is mainly used for people who want their personal computers.&lt;/s&gt;</td>\n","      <td>2.463455</td>\n","      <td>2.451463</td>\n","      <td>-0.011992</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>Summarize the following conversation. #Person1#: Here is the final draft of our contract. I'm glad that we have reached an agreement on almost every term in our trade. #Person2#: Yes, it seems to me we have come quite a long way. However, let me take a close look at the final draft. #Person1#: Do you have some points to bring up? #Person2#: Well, everything we've discussed seems to be here. #Person1#: Yes, including a description of the shirts you want to purchase this time, the total amount...</td>\n","      <td>&lt;pad&gt; #Person2# likes the final draft of their contract, because #Person1# believes the contract has reached an agreement on almost every term in the trade. #Person1# then compares the final draft of the contract with the sample 25 shirts and displays the rules that set the standard.&lt;/s&gt;</td>\n","      <td>&lt;pad&gt; #Person2# looks at the final draft of the contract and offers some notes on each detail in it. #Person2# may sign the contract right now.&lt;/s&gt;</td>\n","      <td>3.213264</td>\n","      <td>3.168305</td>\n","      <td>-0.044959</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>Summarize the following conversation. #Person1#: How much are you asking for this? #Person2#: I'm offering them to you at 150 yuan a piece. Is that all right? #Person1#: Is tax already included in their price? #Person2#: Yes. Our price can't be matched. #Person1#: Would you consider a volume discount? #Person2#: If you buy 1, 000 or more, you'll get a 10 % discount. #Person1#: I'll accept your offer. Summary: &lt;/s&gt;</td>\n","      <td>&lt;pad&gt; #Person2#'s asking for a spend more than 150 yuan by taking deductions for individual purchases. #Person1# accepts the offer.&lt;/s&gt;</td>\n","      <td>&lt;pad&gt; #Person1# asks for the new toothpastes because the price can't be matched. #Person2# offers to chip in a volume discount for the tons of toothpaste.&lt;/s&gt;</td>\n","      <td>2.397067</td>\n","      <td>2.342988</td>\n","      <td>-0.054079</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>Summarize the following conversation. #Person1#: So how did you like the restaurant? #Person2#: Actually, it could have been better. #Person1#: What didn't you like about it? #Person2#: It is a new restaurant. I don't think they have their act together yet. #Person1#: What did you think about the food? #Person2#: I felt that the food was pretty mediocre. #Person1#: The service wasn't that great, either. #Person2#: I agree. The service was not good. #Person1#: Do you think that you want to tr...</td>\n","      <td>&lt;pad&gt; #Person1# asks #Person2# what the restaurant should have been better and felt their food was mediocre. #Person2# says they didn't have the act together yet and #Person2#'s thinking that they'd end up not going again.&lt;/s&gt;</td>\n","      <td>&lt;pad&gt; #Person3# tells #Person1# the restaurant was a good one and the service wasn't so good and they cannot accept it.&lt;/s&gt;</td>\n","      <td>1.959935</td>\n","      <td>1.892986</td>\n","      <td>-0.066949</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>Summarize the following conversation. #Person1#: Could you help me figure out how to look for a job? #Person2#: We have lots of options, what type of job do you need? #Person1#: I want to work in an office. #Person2#: Do you want to work part-time or full-time? #Person1#: I want to work full-time. #Person2#: We have binders with local job listings or you can make use of the computers. OK? #Person1#: I am confused a bit but I am sure that I can figure it out. #Person2#: If you make an appoint...</td>\n","      <td>&lt;pad&gt; #Person1# has a very difficult job search. #Person2# tells #Person1# there is a job center to help #Person1# find the job and can help #Person1#. #Person1# wants to visit a job counselor.&lt;/s&gt;</td>\n","      <td>&lt;pad&gt; #Person1# wants to work full-time in the office. #Person1# needs to work part-time but #Person2# recommends a job counselor.&lt;/s&gt;</td>\n","      <td>2.251136</td>\n","      <td>2.159651</td>\n","      <td>-0.091486</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>Summarize the following conversation. #Person1#: I would like to order some internet today. #Person2#: What kind would you like? #Person1#: What kind of internet is there? #Person2#: You can get DEL or dial-up. #Person1#: Which of those two is best? #Person2#: I would recommend DEL. #Person1#: So that one better? #Person2#: It's better because it doesn't tie up the phone. #Person1#: What do you mean by that? #Person2#: DEL isn't connected through your phone line, but dial-up is. #Person1#: S...</td>\n","      <td>&lt;pad&gt; #Person1# wants to order some lines today but #Person2# recommendd DEL because it makes it easy to use both.&lt;/s&gt;</td>\n","      <td>&lt;pad&gt; #Person1# wants to order some internet. #Person2# recommends dial-up because it gets connected through the phone line, but it can't use the phone.&lt;/s&gt;</td>\n","      <td>2.459294</td>\n","      <td>2.325212</td>\n","      <td>-0.134083</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>Summarize the following conversation. #Person1#: Let's take a coffee break, shall we? #Person2#: I wish I could, but I can't. #Person1#: What keeps you so busy? You've been sitting there for hours. You've got to walk around. You just can't stay on the computer forever. #Person2#: Well, I am up to my neck in work. I've got to finish this report. Sarah needs it by noon. I don't want to be scolded if I can't finish my work by the deadline. #Person1#: I understand that, but you'd feel better if ...</td>\n","      <td>&lt;pad&gt; #Person1# suggests taking a short break and asking #Person2# to stop working so they can clear up the schedule. They already have a meeting to discuss the report's deadline but #Person2# wants one.&lt;/s&gt;</td>\n","      <td>&lt;pad&gt; #Person2# cannot take a coffee break because he is up to his neck in work.&lt;/s&gt;</td>\n","      <td>2.100971</td>\n","      <td>1.683845</td>\n","      <td>-0.417126</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>Summarize the following conversation. #Person1#: Mom, I just finished my paper. Can you proofread it before I hand it in? #Person2#: Sure, let's take a look. Sweetie, this is terrific. Your ideas are so original. #Person1#: Thanks. #Person2#: I can tell you worked hard on it. #Person1#: I really did! I started thinking about what I wanted to say three weeks ago. #Person2#: Well, it was definitely worth all the time. #Person1#: Let's just hope my teacher agrees. Summary: &lt;/s&gt;</td>\n","      <td>&lt;pad&gt; #Person1# can tell Mom that #Person1#'s paper is terrific, and #Person2# says that it was worth all the time for everyone.&lt;/s&gt;</td>\n","      <td>&lt;pad&gt; Speaking to mom, #Person1# wants to show her all the time to #Person1#'s teacher. #Person1# says the papers they worked hard on are wonderful. #Person2# praises her work.&lt;/s&gt;</td>\n","      <td>3.122017</td>\n","      <td>2.484023</td>\n","      <td>-0.637994</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  query  \\\n","0   Summarize the following conversation. #Person1#: It smells like an ashtray in here! #Person2#: Hi honey! What's wrong? Why do you have that look on your face? #Person1#: What's wrong? I thought we agreed that you were gonna quit smoking. #Person2#: No! I said I was going to cut down which is very different. You can't just expect me to go cold turkey overnight! #Person1#: Look, there are other ways to quit. You can try the nicotine patch, or nicotine chewing gum. We spend a fortune on cigaret...   \n","1                                                                                                                                                                   Summarize the following conversation. #Person1#: Judy, what is everybody talking about? #Person2#: Haven't you heard? Richard was fired by our manager. #Person1#: You're kidding. It can't be true. #Person2#: Believe it or not. Everybody is talking about it in the company. #Person1#: Really? I'm surprised. #Person2#: Me too. Summary: </s>   \n","2   Summarize the following conversation. #Person1#: Hello? #Person2#: Hello? #Person1#: Can I speak to Li Hong, please? #Person2#: Speaking. #Person1#: Hi, Li Hong. This is Alice. #Person2#: Hi, Alice. How are you? #Person1#: Not bad. Li Hong, I am sorry that I can't go to see Mrs. Brown with you tomorrow morning. My mother is ill. I must take care of her. #Person2#: I'm sorry to hear that. You'd better stay at home. After all, we can visit Mrs. Brown later #Person1#: OK. Bye - bye. #Person2#: ...   \n","3                                                                                                                                                                                                                                         Summarize the following conversation. #Person1#: Could you help me, Sir? My flight got in 15 minutes ago. Everyone else has picked up the luggage but mine hasn't come through. #Person2#: I'm sorry, Madam, I'll go and find out if there is any more to come. Summary: </s>   \n","4                                                                                                                                                                                                                           Summarize the following conversation. #Person1#: Amanda, how do you like this peaked cap? #Person2#: Didn't you say you want to buy a top hat? #Person1#: But I think this one fits me Well. Why don't you try on the sombrero in black? #Person2#: I don't like caps at all. Summary: </s>   \n","5   Summarize the following conversation. #Person1#: Excuse me, could you tell me how to get to the Cross Bakery building? #Person2#: The Cross Bakery building? Oh sure. You're actually walking in the opposite direction. #Person1#: Oh, you're kidding! I thought I was heading east. #Person2#: No, east is the other direction. To get to the Bakery, you need to turn around and go three blocks to Broadway. When you get to the intersection of Broadway and Elm, you hang a left. Go straight down that st...   \n","6   Summarize the following conversation. #Person1#: Where shall I register, please? #Person2#: Here. Do you have a registration card? #Person1#: Yes. Here you are. #Person2#: Please register your information here and pay for it. And I'll make a medical record for you. #Person1#: OK. How much do I need to pay for the registration? #Person2#: Please pay ten yuan for the registration. #Person1#: Here is my money. #Person2#: This is your registration card. Please don't lose it and bring it whenever...   \n","7   Summarize the following conversation. #Person1#: Hello. I want to reconfirm our flight to London. #Person2#: Yes, sir. Did you call the airline? #Person1#: Yes, I did. But I couldn't communicate with them in English. They speak only Spanish. So I need your help. #Person2#: Certainly, sir. What is the flight number and when are you leaving? #Person1#: We are taking IB 385 to London tomorrow at 1 p. m. #Person2#: Oh, I see, sir. We have the airline office inside the hotel. They have an English...   \n","8   Summarize the following conversation. #Person1#: Oh, my God! What's this? #Person2#: What? #Person1#: Look! This window is open. #Person2#: Did you open it before we left? #Person1#: Are you kidding? It's winter. Why would I open it? #Person2#: I don't know. Wait. Is this yours? #Person1#: No! Oh, my God! Someone has broken into the house. #Person2#: It looks that way. That's probably why the door wasn't locked when we came in. #Person1#: I locked it when I left though. #Person2#: Yes, but t...   \n","9   Summarize the following conversation. #Person1#: I'm forming a music band. #Person2#: Do you already know how to play an instrument? #Person1#: Uh... Yeah! I'Ve told you a thousand times that I'm learning to play the drums. Now that I know how to play well, I would like to form a rock band. #Person2#: Aside from yourself, who are the other members of the band? #Person1#: We have a guy who plays guitar, and another who plays bass. Although we still haven't found anyone to be our singer. You t...   \n","10                                                                                                                                                                        Summarize the following conversation. #Person1#: I'd like to have this cashed, please. #Person2#: Please put you name and address here. May I see your passport? #Person1#: Yes. #Person2#: How would you like it? #Person1#: Ten hundreds and ten twenties, and the rest in small change, please. #Person2#: OK. Here you are. Summary: </s>   \n","11          Summarize the following conversation. #Person1#: What can I do for you, madam? #Person2#: I'd like to buy a toy car for my son. #Person1#: How about this one? #Person2#: It looks nice. How much is it? #Person1#: They're three hundred dollars. #Person2#: Oh, I'm afraid it's too expensive. Can you show me something cheaper? #Person1#: OK, This one is one hundred and twenty. It's the cheapest here. #Person2#: OK, I'll take it. Here's the money. #Person1#: Thank you very much. Summary: </s>   \n","12  Summarize the following conversation. #Person1#: Today more and more families have personal computers. People have wider range of choice to communicate with the outside world. #Person2#: Right. With the establishment of Internet and a lot of web companies, people are getting more and more dependent on the web. #Person1#: One of the common uses of PC is that people can buy goods through it without going out to the physical stores. #Person2#: Can you tell me how it is done? #Person1#: If a cus...   \n","13  Summarize the following conversation. #Person1#: Here is the final draft of our contract. I'm glad that we have reached an agreement on almost every term in our trade. #Person2#: Yes, it seems to me we have come quite a long way. However, let me take a close look at the final draft. #Person1#: Do you have some points to bring up? #Person2#: Well, everything we've discussed seems to be here. #Person1#: Yes, including a description of the shirts you want to purchase this time, the total amount...   \n","14                                                                                    Summarize the following conversation. #Person1#: How much are you asking for this? #Person2#: I'm offering them to you at 150 yuan a piece. Is that all right? #Person1#: Is tax already included in their price? #Person2#: Yes. Our price can't be matched. #Person1#: Would you consider a volume discount? #Person2#: If you buy 1, 000 or more, you'll get a 10 % discount. #Person1#: I'll accept your offer. Summary: </s>   \n","15  Summarize the following conversation. #Person1#: So how did you like the restaurant? #Person2#: Actually, it could have been better. #Person1#: What didn't you like about it? #Person2#: It is a new restaurant. I don't think they have their act together yet. #Person1#: What did you think about the food? #Person2#: I felt that the food was pretty mediocre. #Person1#: The service wasn't that great, either. #Person2#: I agree. The service was not good. #Person1#: Do you think that you want to tr...   \n","16  Summarize the following conversation. #Person1#: Could you help me figure out how to look for a job? #Person2#: We have lots of options, what type of job do you need? #Person1#: I want to work in an office. #Person2#: Do you want to work part-time or full-time? #Person1#: I want to work full-time. #Person2#: We have binders with local job listings or you can make use of the computers. OK? #Person1#: I am confused a bit but I am sure that I can figure it out. #Person2#: If you make an appoint...   \n","17  Summarize the following conversation. #Person1#: I would like to order some internet today. #Person2#: What kind would you like? #Person1#: What kind of internet is there? #Person2#: You can get DEL or dial-up. #Person1#: Which of those two is best? #Person2#: I would recommend DEL. #Person1#: So that one better? #Person2#: It's better because it doesn't tie up the phone. #Person1#: What do you mean by that? #Person2#: DEL isn't connected through your phone line, but dial-up is. #Person1#: S...   \n","18  Summarize the following conversation. #Person1#: Let's take a coffee break, shall we? #Person2#: I wish I could, but I can't. #Person1#: What keeps you so busy? You've been sitting there for hours. You've got to walk around. You just can't stay on the computer forever. #Person2#: Well, I am up to my neck in work. I've got to finish this report. Sarah needs it by noon. I don't want to be scolded if I can't finish my work by the deadline. #Person1#: I understand that, but you'd feel better if ...   \n","19                      Summarize the following conversation. #Person1#: Mom, I just finished my paper. Can you proofread it before I hand it in? #Person2#: Sure, let's take a look. Sweetie, this is terrific. Your ideas are so original. #Person1#: Thanks. #Person2#: I can tell you worked hard on it. #Person1#: I really did! I started thinking about what I wanted to say three weeks ago. #Person2#: Well, it was definitely worth all the time. #Person1#: Let's just hope my teacher agrees. Summary: </s>   \n","\n","                                                                                                                                                                                                                                                                                     response_before  \\\n","0                                                                         <pad> Hopeless honey tells 6061# she has bad rape and #Person1# asks her to quit smoking because she doesn't have the willpower to do so. She said she'll keep going, but #Person1# tells her she will need a divorce.</s>   \n","1                                                                                                                                                                                   <pad> Judy's surprised by the news about Richard being fired by her manager. She tells Judy she's surprised.</s>   \n","2                                                                                                                                                                                      <pad> Alice's mother's ill so she can't go to Mrs. Brown with Li Hong. Li Hong tells her to stay at home.</s>   \n","3                                                                                                                              <pad> #Person1# asks a man for help and the flight is 60 minutes delayed. They share the situation. #Person2# decides to go and see if there is any more to come.</s>   \n","4                                                                                                                                                                                                          <pad> Amanda loves a peaked cap. Then, Amanda asks #Person2# for a sombrero in black.</s>   \n","5                                                                <pad> #Person1# asks #Person2# to show #Person1# the way to the Cross Bakery walking in the opposite direction. #Person2# tells #Person1# it's the opposite direction and shows #Person1# the way to the Bakery outside Bakery.</s>   \n","6                                                                                                                    <pad> #Person1# asks for a registration. #Person2# gives #Person1# a registration code and in wong Yi from the pharmacy and tells #Person1# the way to the counseling room.</s>   \n","7                                                                                                                          <pad> #Person1# says #Person1# will take an IB 385 to London tomorrow, but can't call the airline in English. They awill ask 38 by calling 275 to confirm the flight.</s>   \n","8      <pad> Allen doubts Allen is old and he doesn't know what masked the burglar. They'll look upstairs to see if anyone had robbed the house. Allen confesses to #Person1# and tells #Person1# the TV's at the window and the food on the table. They do not think there will be someone yet.</s>   \n","9       <pad> #Person1# wants to form a rock band. #Person2# tells #Person1# the main bands are a guy and a singer. All the members of the band have musicians. They make auditions for the rock band and some pictures. #Person2# says #Person1# has some musical talent and invites #Person1#.</s>   \n","10                                                                                                                        <pad> #Person1# wants to cheap car new. #Person2# needs to sell this unavailable car and #Person1# sends in the cash in large change for 10 hundreds and ten twenties.</s>   \n","11                                                                                                                                    <pad> #Person1# wants to buy a toy car for #Person2#'s son, but #Person2#'s afraid the price is too expensive. #Person1# shows #Person2# the cheapest one.</s>   \n","12                                                                                                  <pad> #Person1# tells #Person2# that many families have personal PCs. #Person2# teaches some practical ways for the customers to buy goods through the PC through the customers' invitation.</s>   \n","13  <pad> #Person2# likes the final draft of their contract, because #Person1# believes the contract has reached an agreement on almost every term in the trade. #Person1# then compares the final draft of the contract with the sample 25 shirts and displays the rules that set the standard.</s>   \n","14                                                                                                                                                           <pad> #Person2#'s asking for a spend more than 150 yuan by taking deductions for individual purchases. #Person1# accepts the offer.</s>   \n","15                                                                <pad> #Person1# asks #Person2# what the restaurant should have been better and felt their food was mediocre. #Person2# says they didn't have the act together yet and #Person2#'s thinking that they'd end up not going again.</s>   \n","16                                                                                             <pad> #Person1# has a very difficult job search. #Person2# tells #Person1# there is a job center to help #Person1# find the job and can help #Person1#. #Person1# wants to visit a job counselor.</s>   \n","17                                                                                                                                                                            <pad> #Person1# wants to order some lines today but #Person2# recommendd DEL because it makes it easy to use both.</s>   \n","18                                                                                   <pad> #Person1# suggests taking a short break and asking #Person2# to stop working so they can clear up the schedule. They already have a meeting to discuss the report's deadline but #Person2# wants one.</s>   \n","19                                                                                                                                                              <pad> #Person1# can tell Mom that #Person1#'s paper is terrific, and #Person2# says that it was worth all the time for everyone.</s>   \n","\n","                                                                                                                                                                                                                        response_after  \\\n","0                                                 <pad> #Person1# thinks #Person2# smells like an ashtray because she doesn't know how to quit smoking and is too stressed to quit. #Person1# treats the situation embarrassingly.</s>   \n","1                                                                                                                                                                                <pad> Judy wonders what people are talking about.</s>   \n","2                                                                                    <pad> Diane tells Li Hong that she can't go to see Mrs. Brown with Li Hong because she has a severe illness and is taking care of her mother.</s>   \n","3                                   <pad> #Person1#'s flight got in 15 minutes ago, but while everyone else chipped in their luggage, #Person1#'s flight hasn't come through. #Person2#'ll ask her if there is anything else left.</s>   \n","4                                                                                                                                                      <pad> Amanda showed #Person1# a peaked cap by #1 and she prefers a top hat.</s>   \n","5   <pad> #Person1# asks #Person2# how to get to the Cross Bakery building. To get to the Bakery, you have to walk three blocks to Broadway from Elm and turn right. To continue, you'd have to make a left when you hit Broadway.</s>   \n","6                                                                                                                  <pad> #Person1# goes to the counseling room and is informed by #Person2# how to approach the consultation room.</s>   \n","7                                          <pad> #Person1# calls a travel agency for a flight to London to make his request. #Person2# tells #Person1# #Person2# has informed the airline and they have an English-speaking staff.</s>   \n","8                   <pad> Allen thinks somebody has broken the house and doesn't want to let the robber in so he locked it before leaving. Allen thinks he forgot to lock the window the first time so he forgot to lock the door.</s>   \n","9           <pad> #Person1# wants to form a music band. #Person1# wants to make a rock band but they don't have enough room. #Person2# wants to audition this weekend. Besides fraction, they have acoustics and are not a singer.</s>   \n","10                                                                                                                 <pad> #Person1# wants to have this camshed and pay in small change. #Person2# has #Person1#'s name and address.</s>   \n","11                                                                                                                                                                    <pad> #Person1# offers #Person2# a toy car at a cheap price.</s>   \n","12                                                                                   <pad> #Person1# tells #Person2# how people can buy computers through the web, it is mainly used for people who want their personal computers.</s>   \n","13                                                                                 <pad> #Person2# looks at the final draft of the contract and offers some notes on each detail in it. #Person2# may sign the contract right now.</s>   \n","14                                                                      <pad> #Person1# asks for the new toothpastes because the price can't be matched. #Person2# offers to chip in a volume discount for the tons of toothpaste.</s>   \n","15                                                                                                         <pad> #Person3# tells #Person1# the restaurant was a good one and the service wasn't so good and they cannot accept it.</s>   \n","16                                                                                              <pad> #Person1# wants to work full-time in the office. #Person1# needs to work part-time but #Person2# recommends a job counselor.</s>   \n","17                                                                        <pad> #Person1# wants to order some internet. #Person2# recommends dial-up because it gets connected through the phone line, but it can't use the phone.</s>   \n","18                                                                                                                                                <pad> #Person2# cannot take a coffee break because he is up to his neck in work.</s>   \n","19                                                <pad> Speaking to mom, #Person1# wants to show her all the time to #Person1#'s teacher. #Person1# says the papers they worked hard on are wonderful. #Person2# praises her work.</s>   \n","\n","    reward_before  reward_after  reward_diff  \n","0        0.559593      1.392192     0.832600  \n","1        1.384994      1.858945     0.473952  \n","2        1.260455      1.635229     0.374774  \n","3        2.153149      2.422932     0.269783  \n","4        1.318374      1.578068     0.259693  \n","5        2.957855      3.076255     0.118400  \n","6        1.515073      1.623435     0.108362  \n","7        1.671294      1.769185     0.097891  \n","8        2.079719      2.171317     0.091598  \n","9        2.842946      2.930702     0.087757  \n","10       1.692237      1.738544     0.046307  \n","11       1.333509      1.363199     0.029690  \n","12       2.463455      2.451463    -0.011992  \n","13       3.213264      3.168305    -0.044959  \n","14       2.397067      2.342988    -0.054079  \n","15       1.959935      1.892986    -0.066949  \n","16       2.251136      2.159651    -0.091486  \n","17       2.459294      2.325212    -0.134083  \n","18       2.100971      1.683845    -0.417126  \n","19       3.122017      2.484023    -0.637994  "]},"metadata":{}}]},{"cell_type":"markdown","source":"## References\n\nThe creation of this document was greatly influenced by the following key sources of information:\n\n1. [DialogSum Dataset](https://huggingface.co/datasets/knkarthick/dialogsum) DialogSum is a large-scale dialogue summarization dataset, consisting of 13,460 (Plus 100 holdout data for topic generation) dialogues with corresponding manually labeled summaries and topics.\n2. [Generative AI with Large Language Models | Coursera](https://www.coursera.org/learn/generative-ai-with-llms?utm_medium=sem&utm_source=gg&utm_campaign=B2C_NAMER_generative-ai-with-llms_deeplearning-ai_FTCOF_learn_country-US-country-CA&campaignid=20534248984&adgroupid=160068579824&device=c&keyword=&matchtype=&network=g&devicemodel=&adposition=&creativeid=673251286004&hide_mobile_promo&gclid=CjwKCAjwg4SpBhAKEiwAdyLwvEW_WnNyptOwzHtsGmn5-OxT5BKsQeUXHPahO-opBJ0JjsSynHkPAxoCaoAQAvD_BwE) - An informative guide that provides in-depth explanations and examples on various LLMs.","metadata":{}}]}